{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "import json\n",
    "\n",
    "APIKEY = '<Your semantic scholar API key>'\n",
    "\n",
    "output_folder = '../data/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following function performs bulk keyword search using the provided keyword. Saves info about papers as .json file. Note - Sometimes stops earlier than expected. A simple fix is to delete the output and try again until it works. This happens because the token to continue the search sometimes is not returned by the API for some reason"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bulk_papers(query,maximum=-1):\n",
    "\n",
    "    # Define the API endpoint URL\n",
    "    url = \"http://api.semanticscholar.org/graph/v1/paper/search/bulk\"\n",
    "\n",
    "    # Define the query parameters\n",
    "    query_params = {\n",
    "        \"query\": query,\n",
    "        \"fields\": \"title,year,abstract,externalIds\",\n",
    "        'publicationDateOrYear': \"2010:2017\"\n",
    "    }\n",
    "\n",
    "    # Directly define the API key (Reminder: Securely handle API keys in production environments)\n",
    "    api_key = APIKEY  # Replace with the actual API key\n",
    "\n",
    "    # Define headers with API key\n",
    "    headers = {\"x-api-key\": api_key}\n",
    "\n",
    "    # Send the API request\n",
    "    response = requests.get(url, params=query_params, headers=headers).json()\n",
    "\n",
    "    print(f\"Will retrieve an estimated {response['total']} documents\")\n",
    "    total = response['total']\n",
    "    retrieved = 0\n",
    "    if maximum == -1:\n",
    "        maximum = total\n",
    "    # Write results to json file and get next batch of results\n",
    "    with open(f\"{output_folder}search_results_{query.replace(' ', '_').replace('|', 'or')}.json\", 'a') as file:\n",
    "        while retrieved < total and retrieved < maximum:\n",
    "            if \"data\" in response:\n",
    "                retrieved += len(response[\"data\"])\n",
    "                print(f\"Retrieved {retrieved} papers...\")\n",
    "                for paper in response[\"data\"]:\n",
    "                    print(json.dumps(paper), file=file)\n",
    "            # checks for continuation token to get next batch of results\n",
    "            if \"token\" not in response:\n",
    "                break\n",
    "            query_params = {\n",
    "                \"query\": query,\n",
    "                \"fields\": \"title,year,abstract,externalIds\",\n",
    "                'publicationDateOrYear': \"2010:2017\",\n",
    "                'token': response['token']\n",
    "            }\n",
    "            response = requests.get(url, params=query_params, headers=headers).json()\n",
    "    print(f\"Done! Retrieved {retrieved} papers total\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then use the json file to get a csv containing the information we need later in the right format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def json_to_csv(query):\n",
    "    file_path = f\"{output_folder}search_results_{query.replace(' ', '_').replace('|', 'or')}.json\"\n",
    "\n",
    "# List to store the parsed JSON objects\n",
    "    papers = []\n",
    "    result = pd.DataFrame(columns=['DOI', 'Year', 'Title', 'Abstract'])\n",
    "# Read the file line by line\n",
    "    with open(file_path, 'r') as file:\n",
    "        for line in file:\n",
    "            # Strip any extra whitespace or newline characters\n",
    "            line = line.strip()\n",
    "            if line:  # Ensure the line is not empty\n",
    "                try:\n",
    "                    # Parse the JSON object and append it to the list\n",
    "                    json_object = json.loads(line)\n",
    "                    papers.append(json_object)\n",
    "                except json.JSONDecodeError as e:\n",
    "                    print(f\"Error decoding JSON: {e} for line: {line}\")\n",
    "    \n",
    "    for paper in papers:\n",
    "        doi = ''\n",
    "        if 'DOI' in paper['externalIds']:\n",
    "            doi = paper['externalIds']['DOI']\n",
    "        new_row = {'DOI': doi, 'Year': paper['year'], 'Title': paper['title'], 'Abstract': paper['abstract']}\n",
    "        if not pd.isna(new_row['Abstract']):\n",
    "            result.loc[len(result)]= new_row\n",
    "\n",
    "    result.to_csv(f\"{output_folder}search_results_{query.replace(' ', '_').replace('|', 'or')}.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The two following functions compare the search results to saeki's dataset using DOI. There is one function if the search results are stored as json and another if they are csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "saeki_papers = pd.read_csv('../data/Saeki_papers_doi.csv', encoding = \"ISO-8859-1\")\n",
    "\n",
    "def compare_csv_with_saeki(query):\n",
    "    search = pd.read_csv(f\"{output_folder}search_results_{query.replace(' ', '_').replace('|', 'or')}.csv\", encoding = \"ISO-8859-1\")\n",
    "\n",
    "    saeki_DOI = saeki_papers['doi'].to_list()\n",
    "    cleaned_list = [x for x in saeki_DOI if not pd.isna(x)]\n",
    "    search_DOI = search['DOI'].to_list()\n",
    "\n",
    "    count =0\n",
    "    for doi in search_DOI:\n",
    "        if doi in cleaned_list:\n",
    "            count += 1\n",
    "\n",
    "    num = len(search_DOI)\n",
    "    print(f\"The search term \\\"{query}\\\" gave {num} results\" )\n",
    "    print(f\"{count} of the results were in Saekis original dataset\")\n",
    "\n",
    "def compare_json_with_saeki(query):\n",
    "    file_path = f\"{output_folder}search_results_{query.replace(' ', '_').replace('|', 'or')}.json\"\n",
    "\n",
    "# List to store the parsed JSON objects\n",
    "    papers = []\n",
    "\n",
    "# Read the file line by line\n",
    "    with open(file_path, 'r') as file:\n",
    "        for line in file:\n",
    "            # Strip any extra whitespace or newline characters\n",
    "            line = line.strip()\n",
    "            if line:  # Ensure the line is not empty\n",
    "                try:\n",
    "                    # Parse the JSON object and append it to the list\n",
    "                    json_object = json.loads(line)\n",
    "                    papers.append(json_object)\n",
    "                except json.JSONDecodeError as e:\n",
    "                    print(f\"Error decoding JSON: {e} for line: {line}\")\n",
    "        \n",
    "    saeki_DOI = saeki_papers['doi'].to_list()\n",
    "    cleaned_list = [x for x in saeki_DOI if not pd.isna(x)]\n",
    "    \n",
    "\n",
    "    count =0\n",
    "    for paper in papers:\n",
    "        if 'DOI' in paper['externalIds']:\n",
    "            doi = paper['externalIds']['DOI']\n",
    "            if doi in cleaned_list:\n",
    "                count += 1\n",
    "\n",
    "    num = len(papers)\n",
    "    print(f\"The search term \\\"{query}\\\" gave {num} results\" )\n",
    "    print(f\"{count} of the results were in Saekis original dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Will retrieve an estimated 121012 documents\n",
      "Retrieved 1000 papers...\n",
      "Retrieved 2000 papers...\n",
      "Retrieved 2999 papers...\n",
      "Retrieved 3998 papers...\n",
      "Retrieved 4998 papers...\n",
      "Retrieved 5998 papers...\n",
      "Done! Retrieved 5998 papers total\n"
     ]
    }
   ],
   "source": [
    "get_bulk_papers('electrolyte', 5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_to_csv('electrolyte')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The search term \"polymer solar cells\" gave 12903 results\n",
      "131 of the results were in Saekis original dataset\n"
     ]
    }
   ],
   "source": [
    "compare_csv_with_saeki('polymer solar cells')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The search term \"photovoltaic polymer\" gave 7579 results\n",
      "76 of the results were in Saekis original dataset\n"
     ]
    }
   ],
   "source": [
    "compare_json_with_saeki('photovoltaic polymer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The search term \"(photovoltaic polymer efficiency) | (polymer solar cell efficiency)\" gave 7602 results\n",
      "121 of the results were in Saekis original dataset\n"
     ]
    }
   ],
   "source": [
    "compare_json_with_saeki('(photovoltaic polymer efficiency) | (polymer solar cell efficiency)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_to_csv('(photovoltaic polymer efficiency) | (polymer solar cell efficiency)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The search term \"(photovoltaic polymer) | (polymer solar cell)\" gave 16310 results\n",
      "152 of the results were in Saekis original dataset\n"
     ]
    }
   ],
   "source": [
    "compare_json_with_saeki('(photovoltaic polymer) | (polymer solar cell)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
